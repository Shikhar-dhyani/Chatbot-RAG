{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "# Import necessary libraries for document processing, vector embeddings, and interaction with Pinecone\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "from langchain_community.vectorstores import Pinecone\n",
    "\n",
    "from pinecone import Pinecone, PodSpec\n",
    "from langchain_core.documents.base import Document\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total PDFs loaded: 4\n",
      "Total pages loaded: 43\n",
      "<class 'langchain_core.documents.base.Document'>\n"
     ]
    }
   ],
   "source": [
    "pdf_directory='Policies/'\n",
    "files = os.listdir(pdf_directory)\n",
    "\n",
    "# Filter for PDF files\n",
    "pdf_files = [file for file in files if file.endswith('.pdf')]\n",
    "\n",
    "# Initialize an empty list to hold all pages from all PDFs\n",
    "all_pages = []\n",
    "\n",
    "# Load and split each PDF\n",
    "for pdf_file in pdf_files:\n",
    "    pdf_path = os.path.join(pdf_directory, pdf_file)\n",
    "    pdf_loader = PyPDFLoader(pdf_path)\n",
    "    pages = pdf_loader.load_and_split()\n",
    "    all_pages.extend(pages)  # Add the pages from the current PDF to the list of all pages\n",
    "\n",
    "# Now `all_pages` contains pages from all PDFs in the directory\n",
    "print(f'Total PDFs loaded: {len(pdf_files)}')\n",
    "print(f'Total pages loaded: {len(all_pages)}')\n",
    "print(type(all_pages[0]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine page contents into a single context string for processing\n",
    "context = \"\\n\".join(str(p.page_content) for p in all_pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the combined context into manageable chunks for embedding generation\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=3200, chunk_overlap=400)\n",
    "texts = text_splitter.split_text(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "def generate_embeddings(texts):\n",
    "    model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "    \"\"\"\n",
    "    Generate embeddings for a list of texts using the SentenceTransformer model.\n",
    "    \n",
    "    Parameters:\n",
    "    texts (list of str): A list of sentences for which to generate embeddings.\n",
    "    \n",
    "    Returns:\n",
    "    np.ndarray: A NumPy array of shape (n_texts, embedding_size) containing the sentence embeddings.\n",
    "    \"\"\"\n",
    "    # The encode method directly returns the embeddings as a NumPy array\n",
    "    embeddings = model.encode(texts)\n",
    "    return embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pinecone_client = Pinecone(api_key=PINECONE_API_KEY)\n",
    "pinecone_index = pinecone_client.Index(\"demo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map document IDs to texts and upsert embeddings into Pinecone\n",
    "id_to_text = {}  # Dictionary to map IDs to texts\n",
    "for i, text in enumerate(texts):\n",
    "    embedding_list = generate_embeddings([text])[0].tolist()\n",
    "    document_id = str(i)\n",
    "    pinecone_index.upsert(vectors=[(document_id, embedding_list)])\n",
    "    id_to_text[document_id] = text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_text=\"What are the exceptions to Work from home policy?\"\n",
    "query_embedding = generate_embeddings([query_text])[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "query_results = pinecone_index.query(vector=query_embedding, top_k=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "    Document(\n",
    "        page_content=id_to_text.get(match['id'], \"Content not found\"),\n",
    "        metadata={\"score\": match['score'], \"page\": match['id']}\n",
    "    )\n",
    "    for match in query_results[\"matches\"]\n",
    "    if match['id'] in id_to_text  # Ensure the ID exists in id_to_text\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine page contents into a single context string for processing\n",
    "context = \"\\n\".join(str(p.page_content) for p in documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Policy Elements  \\n● All employees will be required to work for a period of 3 day s/ Week  from the office pr emises or a Total \\nof at least 13 working days  from office  premises  in a month.   \\n● Additionally,  an employee can avail an extra pre -approved 1 -day WFH  in a week  by the  approval  of \\nRespective  Manager and it complies with the 13 day / month condition above . \\n● Any exception to the rules  above will have to be approved by the Department Heads.  \\nRequesting Work from Home Procedure  \\nWhen employees plan to work from home, this procedure must be followed:  \\n● Employees file a request through email  to Respective Manager with CC to HR, at least two da ys in \\nadvance . The Managers must approve their request considering all elements we mentioned above.  \\nDisclaimer  \\nThe Company may periodically monitor, review and evaluate the working and efficacy of this  policy, and modify / \\nmake improvements in its working, as may be considered appropriate and in  accordance with the prevalent best \\npractices.\\nVersion Contr ol \\n \\nVersion No  Published Date  Prepared By  Significant Changes  \\nRelease 1.0  28th April,22  HR & Management  Initial Release' metadata={'score': 0.570879221, 'page': '22'}\n"
     ]
    }
   ],
   "source": [
    "print(documents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, BartForConditionalGeneration\n",
    "\n",
    "model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Employees will be required to work for a period of 3 day s/ Week from the office pr emises. Any exception to the rules  above will have to be approved by the Department Heads. The Company may periodically monitor, review and evaluate the working and efficacy of this policy.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer([context], return_tensors=\"pt\")\n",
    "\n",
    "# Generate Summary\n",
    "summary_ids = model.generate(inputs[\"input_ids\"], num_beams=2)\n",
    "tokenizer.batch_decode(summary_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
